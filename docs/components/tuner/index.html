<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta property="og:title" content="Tuner" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://finetuner.jina.ai/components/tuner.html" />
  <meta property="og:site_name" content="Finetuner 0.3.1 Documentation" />
  <meta property="og:description" content="Tuner is one of the three key components of Finetuner. Given an embedding model and labeled dataset(see the guide on data formats for more information), Tuner trains the model to fit the data. With Tuner, you can customize the training process to best fit your data, and track your experiements in..." />
  <meta property="og:image" content="https://finetuner.jina.ai/_static/banner.png" />
  <meta property="og:image:alt" content="Finetuner 0.3.1 Documentation" />
  <meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@JinaAI_">
<meta name="twitter:creator" content="@JinaAI_">
<meta name="description" content="Finetuner allows one to finetune any deep neural network for better embedding on search tasks.">
<meta property="og:description" content="Finetuner allows one to finetune any deep neural network for better embedding on search tasks.">

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1ESRNDCK35"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1ESRNDCK35');
</script>
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
<script async defer src="https://cdn.jsdelivr.net/npm/qabot@0.3"></script>
    <link rel="index" title="Index" href="../../genindex/" /><link rel="search" title="Search" href="../../search/" /><link rel="next" title="Loss and Miners" href="loss/" /><link rel="prev" title="Overview" href="../overview/" />

    <link rel="shortcut icon" href="../../_static/favicon.png"/><meta name="generator" content="sphinx-4.4.0, furo 2022.01.02"/>
        <title>Tuner - Finetuner 0.3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=df49af52631e7917044a9c21a57f7b83170a6dd0" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.59c74d8c95b765a7fd995ac71d459ebe.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=fade93df149f7c5fedb3ff897f799dc7d283b420" />
    <link rel="stylesheet" type="text/css" href="../../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/docbot.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: #4d4d4d;
  --color-brand-primary: #009191;
  --color-brand-content: #009191;
  
  }
  body[data-theme="dark"] {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
  }
  @media (prefers-color-scheme: dark) {
    body:not([data-theme="light"]) {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
    }
  }
</style></head>
  <body>
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" />
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
    <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
    <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
    <header class="mobile-header">
        <div class="header-left">
            <label class="nav-overlay-icon" for="__navigation">
                <div class="visually-hidden">Toggle site navigation sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-menu"></use>
                    </svg>
                </i>
            </label>
        </div>
        <div class="header-center">
            <a href="../../">
                <div class="brand">Finetuner 0.3.1 documentation</div>
            </a>
        </div>
        <div class="header-right">
            <div class="theme-toggle-container theme-toggle-header">
                <button class="theme-toggle">
                    <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                    <svg class="theme-icon-when-auto">
                        <use href="#svg-sun-half"></use>
                    </svg>
                    <svg class="theme-icon-when-dark">
                        <use href="#svg-moon"></use>
                    </svg>
                    <svg class="theme-icon-when-light">
                        <use href="#svg-sun"></use>
                    </svg>
                </button>
            </div>
            <label class="toc-overlay-icon toc-header-icon" for="__toc">
                <div class="visually-hidden">Toggle table of contents sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-toc"></use>
                    </svg>
                </i>
            </label>
        </div>
    </header>
    <aside class="sidebar-drawer">
        <div class="sidebar-container">
            
            <div class="sidebar-sticky"><a class="sidebar-brand" href="../../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo-light.svg" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../../_static/logo-dark.svg" alt="Dark Logo"/>
  </div>
  
  
</a>
<div class="sd-d-flex-row sd-align-major-spaced">
  <a class="github-button" href="https://github.com/jina-ai/finetuner" data-icon="octicon-star" data-show-count="true" aria-label="Star jina-ai/finetuner on GitHub" style="opacity: 0;">Star</a>
  
</div><form class="sidebar-search-container" method="get" action="../../search/" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
    <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/swiss-roll/">Finetuning MLP on Swiss Roll Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/totally-looks-like/">Finetuning ResNet50 on Totally Looks Like Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/3d-mesh/">Finetuning PointConv on ModelNet40 Dataset</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../basics/fit/">One-liner <code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../basics/data-format/">Data Format</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../basics/datasets/class-dataset/">Class Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/datasets/session-dataset/">Session Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../basics/datasets/instance-dataset/">Instance dataset</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../basics/glossary/">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Components</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview/">Overview</a></li>
<li class="toctree-l1 current has-children current-page"><a class="current reference internal" href="#">Tuner</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="loss/">Loss and Miners</a></li>
<li class="toctree-l2"><a class="reference internal" href="callbacks/">Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluation/">Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tailor/">Tailor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Reference</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../api/finetuner/">finetuner package</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../api/finetuner.tailor/">finetuner.tailor package</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tailor.keras/">finetuner.tailor.keras package</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tailor.keras.projection_head/">finetuner.tailor.keras.projection_head module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tailor.paddle/">finetuner.tailor.paddle package</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tailor.paddle.projection_head/">finetuner.tailor.paddle.projection_head module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tailor.pytorch/">finetuner.tailor.pytorch package</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tailor.pytorch.projection_head/">finetuner.tailor.pytorch.projection_head module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tailor.base/">finetuner.tailor.base module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../api/finetuner.tuner/">finetuner.tuner package</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.callback/">finetuner.tuner.callback package</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.base/">finetuner.tuner.callback.base module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.best_model_checkpoint/">finetuner.tuner.callback.best_model_checkpoint module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.early_stopping/">finetuner.tuner.callback.early_stopping module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.evaluation/">finetuner.tuner.callback.evaluation module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.progress_bar/">finetuner.tuner.callback.progress_bar module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.training_checkpoint/">finetuner.tuner.callback.training_checkpoint module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.callback.wandb_logger/">finetuner.tuner.callback.wandb_logger module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.dataset/">finetuner.tuner.dataset package</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.dataset.base/">finetuner.tuner.dataset.base module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.dataset.datasets/">finetuner.tuner.dataset.datasets module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.dataset.samplers/">finetuner.tuner.dataset.samplers module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.keras/">finetuner.tuner.keras package</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.keras.data/">finetuner.tuner.keras.data module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.keras.losses/">finetuner.tuner.keras.losses module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.keras.miner/">finetuner.tuner.keras.miner module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.miner/">finetuner.tuner.miner package</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.miner.base/">finetuner.tuner.miner.base module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.miner.mining_strategies/">finetuner.tuner.miner.mining_strategies module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.paddle/">finetuner.tuner.paddle package</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.paddle.datasets/">finetuner.tuner.paddle.datasets module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.paddle.losses/">finetuner.tuner.paddle.losses module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.paddle.miner/">finetuner.tuner.paddle.miner module</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../api/finetuner.tuner.pytorch/">finetuner.tuner.pytorch package</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.pytorch.datasets/">finetuner.tuner.pytorch.datasets module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.pytorch.losses/">finetuner.tuner.pytorch.losses module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/finetuner.tuner.pytorch.miner/">finetuner.tuner.pytorch.miner module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tuner.augmentation/">finetuner.tuner.augmentation module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tuner.base/">finetuner.tuner.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tuner.evaluation/">finetuner.tuner.evaluation module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tuner.onnx/">finetuner.tuner.onnx module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/finetuner.tuner.state/">finetuner.tuner.state module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api/finetuner.embedding/">finetuner.embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/finetuner.excepts/">finetuner.excepts module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/finetuner.helper/">finetuner.helper module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/finetuner.toydata/">finetuner.toydata module</a></li>
</ul>
</li>
</ul>

    <p class="caption" role="heading"><span class="caption-text">Ecosystem</span></p>
    <ul>
        <li class="toctree-l1">
            <a class="reference external" href="https://docs.jina.ai">
                <img class="sidebar-ecosys-logo only-light-line" src="../../_static/search-light.svg">
                <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/search-dark.svg">
                Jina</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://hub.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/hub-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/hub-dark.svg">
            Jina Hub</a></li>
        <li class="toctree-l1"><a class="reference internal" href="#">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/finetuner-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/finetuner-dark.svg">
            Finetuner</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://docarray.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/docarray-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/docarray-dark.svg">
            DocArray</a></li>
    </ul>
</div>
</div>
            </div>
            
        </div>
    </aside>
    <div class="main">
        <div class="content">
            <article role="main">
                <div class="content-icon-container">
                    <div class="theme-toggle-container theme-toggle-content">
                        <button class="theme-toggle">
                            <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                            <svg class="theme-icon-when-auto">
                                <use href="#svg-sun-half"></use>
                            </svg>
                            <svg class="theme-icon-when-dark">
                                <use href="#svg-moon"></use>
                            </svg>
                            <svg class="theme-icon-when-light">
                                <use href="#svg-sun"></use>
                            </svg>
                        </button>
                    </div>
                    <label class="toc-overlay-icon toc-content-icon"
                           for="__toc">
                        <div class="visually-hidden">Toggle table of contents sidebar</div>
                        <i class="icon">
                            <svg>
                                <use href="#svg-toc"></use>
                            </svg>
                        </i>
                    </label>
                </div>
                <section class="tex2jax_ignore mathjax_ignore" id="tuner">
<h1>Tuner<a class="headerlink" href="#tuner" title="Permalink to this headline">¶</a></h1>
<p>Tuner is one of the three key components of Finetuner. Given an <a class="reference internal" href="../../basics/glossary/#term-Embedding-model"><span class="xref std std-term">embedding model</span></a> and <span class="xref std std-term">labeled dataset</span> (see <a class="reference internal" href="../../basics/data-format/#data-format"><span class="std std-ref">the guide on data formats</span></a> for more information), Tuner trains the model to fit the data.</p>
<p>With Tuner, you can customize the training process to best fit your data, and track your experiements in a clear and transparent manner. You can do things like</p>
<ul class="simple">
<li><p>Choose between different loss functions, use hard negative mining for triplets/pairs</p></li>
<li><p>Set your own optimizers and learning rates</p></li>
<li><p>Track the training and evaluation metrics with Weights and Biases</p></li>
<li><p>Save checkpoints during training</p></li>
<li><p>Write custom callbacks</p></li>
</ul>
<p>As part of the training process, you can also compute IR related evaluation metrics, using the standalone
<a class="reference internal" href="../../api/finetuner.tuner.evaluation/#finetuner.tuner.evaluation.Evaluator" title="finetuner.tuner.evaluation.Evaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evaluator</span></code></a> component.</p>
<p>You can read more on these different options here or in these sub-sections:</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="loss/">Loss and Miners</a></li>
<li class="toctree-l1"><a class="reference internal" href="callbacks/">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation/">Evaluation</a></li>
</ul>
</div>
<section id="the-tuner-class">
<h2>The <code class="docutils literal notranslate"><span class="pre">Tuner</span></code> class<a class="headerlink" href="#the-tuner-class" title="Permalink to this headline">¶</a></h2>
<p>All the functionality is exposed through the base <code class="docutils literal notranslate"><span class="pre">*Tuner</span></code> class - <code class="docutils literal notranslate"><span class="pre">PytorchTuner</span></code>, <code class="docutils literal notranslate"><span class="pre">KerasTuner</span></code> and <code class="docutils literal notranslate"><span class="pre">PaddleTuner</span></code>. This class instance also gets constructed under the hood when you call <code class="docutils literal notranslate"><span class="pre">finetuner.fit()</span></code>.</p>
<p>When initializing a <code class="docutils literal notranslate"><span class="pre">*Tuner</span></code> class, you are required to pass the <a class="reference internal" href="../../basics/glossary/#term-Embedding-model"><span class="xref std std-term">embedding model</span></a>, but you can also customize other training configuration.</p>
<p>You can then finetune your model using the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method, to which you pass the training and evaluation data (which should both be <span class="xref std std-term">labeled dataset</span>), as well as any other data-related configuration (see ).</p>
<p>A minimal example looks like this:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"/><label class="tab-label" for="tab-set--0-input--1">PyTorch</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">finetuner.toydata</span> <span class="kn">import</span> <span class="n">generate_fashion</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch</span> <span class="kn">import</span> <span class="n">PytorchTuner</span>

<span class="n">embed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PytorchTuner</span><span class="p">(</span><span class="n">embed_model</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">generate_fashion</span><span class="p">())</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"/><label class="tab-label" for="tab-set--0-input--2">Keras</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">finetuner.toydata</span> <span class="kn">import</span> <span class="n">generate_fashion</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.keras</span> <span class="kn">import</span> <span class="n">KerasTuner</span>

<span class="n">embed_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">KerasTuner</span><span class="p">(</span><span class="n">embed_model</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">generate_fashion</span><span class="p">())</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--3" name="tab-set--0" type="radio"/><label class="tab-label" for="tab-set--0-input--3">Paddle</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">paddle</span>
<span class="kn">from</span> <span class="nn">finetuner.toydata</span> <span class="kn">import</span> <span class="n">generate_fashion</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.paddle</span> <span class="kn">import</span> <span class="n">PaddleTuner</span>

<span class="n">embed_model</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">paddle</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PaddleTuner</span><span class="p">(</span><span class="n">embed_model</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">generate_fashion</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<section id="customize-optimization">
<h3>Customize optimization<a class="headerlink" href="#customize-optimization" title="Permalink to this headline">¶</a></h3>
<p>You can provide your own optimizer and learning rate scheduler if you wish to (by default, the Adam optimizer with a fixed learning rate will be used), using the <code class="docutils literal notranslate"><span class="pre">configure_optimizer</span></code> argument to the Tuner constructor.</p>
<p>For Pytorch and PaddlePaddle, you can also use the <code class="docutils literal notranslate"><span class="pre">scheduler_step</span></code> argument, to set whether to step the learning rate scheduler on each batch or each epoch (for Keras this is not available, there you set the frequency, in terms of batches, in the scheduler itself)</p>
<p>Here’s an example of how you can do this</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"/><label class="tab-label" for="tab-set--1-input--1">Pytorch</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">MultiStepLR</span>

<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch</span> <span class="kn">import</span> <span class="n">PytorchTuner</span>

<span class="k">def</span> <span class="nf">configure_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PytorchTuner</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span> <span class="n">configure_optimizer</span><span class="o">=</span><span class="n">configure_optimizer</span><span class="p">,</span> <span class="n">scheduler_step</span><span class="o">=</span><span class="s1">'epoch'</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"/><label class="tab-label" for="tab-set--1-input--2">Keras</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">finetuner.tuner.keras</span> <span class="kn">import</span> <span class="n">KerasTuner</span>

<span class="k">def</span> <span class="nf">configure_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">ExponentialDecay</span><span class="p">(</span>
            <span class="mf">1.0</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">KerasTuner</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">configure_optimizer</span><span class="o">=</span><span class="n">configure_optimizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--3" name="tab-set--1" type="radio"/><label class="tab-label" for="tab-set--1-input--3">Paddle</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">paddle</span> <span class="kn">import</span> <span class="n">optimizer</span>

<span class="kn">from</span> <span class="nn">finetuner.tuner.paddle</span> <span class="kn">import</span> <span class="n">PaddleTuner</span>

<span class="k">def</span> <span class="nf">configure_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">lr</span><span class="o">.</span><span class="n">MultiStepDecay</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PaddleTuner</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span> <span class="n">configure_optimizer</span><span class="o">=</span><span class="n">configure_optimizer</span><span class="p">,</span> <span class="n">scheduler_step</span><span class="o">=</span><span class="s1">'epoch'</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="saving-the-model">
<h3>Saving the model<a class="headerlink" href="#saving-the-model" title="Permalink to this headline">¶</a></h3>
<p>After a model is tuned, you can save it by calling <code class="docutils literal notranslate"><span class="pre">.save(save_path)</span></code> method.</p>
</section>
</section>
<section id="example-full-training">
<h2>Example - full training<a class="headerlink" href="#example-full-training" title="Permalink to this headline">¶</a></h2>
<p>In the example below we’ll demonstrate how to make full use of the available Tuner features, as you would in any realistic setting.</p>
<p>We will be finetuning a simple MLP model on the Fashion MNIST data, and we will be using:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../api/finetuner.tuner.pytorch.losses/#finetuner.tuner.pytorch.losses.TripletLoss" title="finetuner.tuner.pytorch.losses.TripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">TripletLoss</span></code></a> with easy positive and semihard negative mining strategy</p></li>
<li><p>A custom learning rate schedule</p></li>
<li><p>Tracking the experiement on Weights and Biases using <a class="reference internal" href="../../api/finetuner.tuner.callback.wandb_logger/#finetuner.tuner.callback.wandb_logger.WandBLogger" title="finetuner.tuner.callback.wandb_logger.WandBLogger"><code class="xref py py-class docutils literal notranslate"><span class="pre">WandBLogger</span></code></a> callback</p></li>
<li><p>Random augmentation using <code class="docutils literal notranslate"><span class="pre">preproces_fn</span></code></p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Before trying out the example, make sure you have <a class="reference external" href="https://docs.wandb.ai/quickstart">wandb installed</a> and have logged into your account.</p>
</div>
<p>Let’s start with the dataset - we’ll use the <a class="reference internal" href="../../api/finetuner.toydata/#finetuner.toydata.generate_fashion" title="finetuner.toydata.generate_fashion"><code class="xref py py-meth docutils literal notranslate"><span class="pre">generate_fashion()</span></code></a> helper function, which will produce a <a class="reference internal" href="../../basics/datasets/class-dataset/#class-dataset"><span class="std std-ref">Class Dataset</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">finetuner.toydata</span> <span class="kn">import</span> <span class="n">generate_fashion</span>
<span class="kn">from</span> <span class="nn">docarray</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">generate_fashion</span><span class="p">()</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">generate_fashion</span><span class="p">(</span><span class="n">is_testset</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_fn</span><span class="p">(</span><span class="n">doc</span><span class="p">:</span> <span class="n">Document</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Add some noise to the image"""</span>
    <span class="n">new_image</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">tensor</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Size of train data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Size of eval data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Example of label: </span><span class="si">{</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tags</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Example of tensor: </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> shape, type </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Size of train data: 60000                                                                           </span>
<span class="go">Size of train data: 10000</span>
<span class="go">Example of label: {</span>
<span class="go">  "finetuner_label": 9.0</span>
<span class="go">}</span>
<span class="go">Example of tensor: (28, 28) shape, type float32</span>
</pre></div>
</div>
<p>Next, we prepare the model - just a simple MLP in this case</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">embed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Then we can create the <a class="reference internal" href="../../api/finetuner.tuner.pytorch/#finetuner.tuner.pytorch.PytorchTuner" title="finetuner.tuner.pytorch.PytorchTuner"><code class="xref py py-class docutils literal notranslate"><span class="pre">PytorchTuner</span></code></a> object. In this step we specify all the training configuration. We’ll be using</p>
<ul class="simple">
<li><p>Triplet loss with hard miner with the easy positive and semihard negative strategy</p></li>
<li><p>Adam optimizer with initial learning rate of 0.0005, which will be halved every 30 epochs</p></li>
<li><p>WandB for tracking the experiement</p></li>
<li><p>A <a class="reference internal" href="../../api/finetuner.tuner.callback.training_checkpoint/#finetuner.tuner.callback.training_checkpoint.TrainingCheckpoint" title="finetuner.tuner.callback.training_checkpoint.TrainingCheckpoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainingCheckpoint</span></code></a> to save a checkpoint every epoch - if training is interrupted we can later continue from this checkpoint. We need to create a <code class="docutils literal notranslate"><span class="pre">checkpoints/</span></code> folder inside our current directory to store checkpoints there.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">MultiStepLR</span>

<span class="kn">from</span> <span class="nn">finetuner.tuner.callback</span> <span class="kn">import</span> <span class="n">WandBLogger</span><span class="p">,</span> <span class="n">TrainingCheckpoint</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch</span> <span class="kn">import</span> <span class="n">PytorchTuner</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch.losses</span> <span class="kn">import</span> <span class="n">TripletLoss</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch.miner</span> <span class="kn">import</span> <span class="n">TripletEasyHardMiner</span>


<span class="k">def</span> <span class="nf">configure_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>


<span class="n">loss</span> <span class="o">=</span> <span class="n">TripletLoss</span><span class="p">(</span>
    <span class="n">miner</span><span class="o">=</span><span class="n">TripletEasyHardMiner</span><span class="p">(</span><span class="n">pos_strategy</span><span class="o">=</span><span class="s1">'easy'</span><span class="p">,</span> <span class="n">neg_strategy</span><span class="o">=</span><span class="s1">'semihard'</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">logger_callback</span> <span class="o">=</span> <span class="n">WandBLogger</span><span class="p">()</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">TrainingCheckpoint</span><span class="p">(</span><span class="s1">'checkpoints'</span><span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PytorchTuner</span><span class="p">(</span>
    <span class="n">embed_model</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">configure_optimizer</span><span class="o">=</span><span class="n">configure_optimizer</span><span class="p">,</span>
    <span class="n">scheduler_step</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">logger_callback</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">],</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Finally, let’s put it all together and run the training</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">MultiStepLR</span>

<span class="kn">from</span> <span class="nn">finetuner.toydata</span> <span class="kn">import</span> <span class="n">generate_fashion</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.callback</span> <span class="kn">import</span> <span class="n">WandBLogger</span><span class="p">,</span> <span class="n">TrainingCheckpoint</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch</span> <span class="kn">import</span> <span class="n">PytorchTuner</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch.losses</span> <span class="kn">import</span> <span class="n">TripletLoss</span>
<span class="kn">from</span> <span class="nn">finetuner.tuner.pytorch.miner</span> <span class="kn">import</span> <span class="n">TripletEasyHardMiner</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">generate_fashion</span><span class="p">()</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">generate_fashion</span><span class="p">(</span><span class="n">is_testset</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_fn</span><span class="p">(</span><span class="n">doc</span><span class="p">:</span> <span class="n">Document</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Add some noise to the image"""</span>
    <span class="n">new_image</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">tensor</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">doc</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">embed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">32</span><span class="p">),</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">configure_optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span>


<span class="n">loss</span> <span class="o">=</span> <span class="n">TripletLoss</span><span class="p">(</span>
    <span class="n">miner</span><span class="o">=</span><span class="n">TripletEasyHardMiner</span><span class="p">(</span><span class="n">pos_strategy</span><span class="o">=</span><span class="s1">'easy'</span><span class="p">,</span> <span class="n">neg_strategy</span><span class="o">=</span><span class="s1">'semihard'</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">logger_callback</span> <span class="o">=</span> <span class="n">WandBLogger</span><span class="p">()</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">TrainingCheckpoint</span><span class="p">(</span><span class="s1">'checkpoints'</span><span class="p">)</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">PytorchTuner</span><span class="p">(</span>
    <span class="n">embed_model</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">configure_optimizer</span><span class="o">=</span><span class="n">configure_optimizer</span><span class="p">,</span>
    <span class="n">scheduler_step</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">logger_callback</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">],</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">eval_data</span><span class="p">,</span> <span class="n">preprocess_fn</span><span class="o">=</span><span class="n">preprocess_fn</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">num_items_per_class</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We can monitor the training by watching the progress bar, or we can log into our WanB account, and see the live updates there. Here’s an example of what we might see there</p>
<p><img alt="wandb dashboard" src="../../_images/wandb.png"/></p>
</section>
</section>

            </article>
            <footer>
                
                <div class="related-pages">
                    <a class="next-page" href="loss/">
                        <div class="page-info">
                            <div class="context">
                                <span>Next</span>
                            </div>
                            <div class="title">Loss and Miners</div>
                        </div>
                        <svg>
                            <use href="#svg-arrow-right"></use>
                        </svg>
                    </a>
                    <a class="prev-page" href="../overview/">
                        <svg>
                            <use href="#svg-arrow-right"></use>
                        </svg>
                        <div class="page-info">
                            <div class="context">
                                <span>Previous</span>
                            </div>
                            
                            <div class="title">Overview</div>
                            
                        </div>
                    </a>
                </div>

                <div class="related-information sd-d-inline-flex">
                    <a href="https://jina.ai">Copyright &#169; Jina AI Limited. All rights reserved.</a>
                    Last updated on Jan 26, 2022.
                    <div class="social-btns">
                    <a class='social-btn' href="https://github.com/jina-ai/finetuner/" target="_blank"> <i class="fab fa-github"></i></a>
                    <a class='social-btn' href="https://slack.jina.ai" target="_blank"> <i class="fab fa-slack"></i></a>
                    <a class='social-btn' href="https://youtube.com/c/jina-ai" target="_blank"> <i class="fab fa-youtube"></i></a>
                    <a class='social-btn' href="https://twitter.com/JinaAI_" target="_blank"> <i class="fab fa-twitter"></i></a>
                    <a class='social-btn' href="https://www.linkedin.com/company/jinaai/" target="_blank"> <i class="fab fa-linkedin"></i></a>
                    </div>
                </div>
                
            </footer>
        </div>
        <aside class="toc-drawer">
            

            <div class="toc-sticky toc-scroll">
                
                <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
                </div>
                <div class="toc-tree-container">
                    <div class="toc-tree">
                        <ul>
<li><a class="reference internal" href="#">Tuner</a><ul>
<li><a class="reference internal" href="#the-tuner-class">The <code class="docutils literal notranslate"><span class="pre">Tuner</span></code> class</a><ul>
<li><a class="reference internal" href="#customize-optimization">Customize optimization</a></li>
<li><a class="reference internal" href="#saving-the-model">Saving the model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-full-training">Example - full training</a></li>
</ul>
</li>
</ul>

                    </div>
                </div>
                
                <qa-bot
                    style="position: fixed; width: 15em"
                    theme="follow"
                >
                    <dt>You can ask questions about our docs. Try:</dt>
                    <dd>What is Tailor?</dd>
                    <dd>How can I freeze layers?</dd>
                    <dd>What is the input data format for finetuner?</dd>
                </qa-bot>
            </div>

            
        </aside>

    </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() { 
            document.querySelector("qa-bot").setAttribute("server", "https://finetuner-docsbot.jina.ai");
        });
        </script>
    </body>
</html>